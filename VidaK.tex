\section{Machine Learning and Markov Model in Natural Language Processing}

\vspace{2cm}
    \subsection{Hidden Markov Models}
    
    A Hidden Markov Model is a statistical model which is also used in Machine Learning. It can be used to describe the evolution of observable events that depend on internal factors, which are not directly observable.
    The applications where the HMM (Hidden Markov Model) can be used are cases such as time series data, audio and video data, and text data or Natural Language Processing data. In this part, our main focus is on those applications of NLP where we can use the HMM for better performance of the model, for example, we can use HMM in the Part-Of-Speech tagging.
    
    \vspace{0.5cm}
    \subsection{What is POS-tagging?}
    
    We have learned that the part of speech indicates the function of any word, like what it means in any sentence. There are commonly nine parts of speeches; noun, pronoun, verb, adverb, article, adjective, preposition, conjunction, interjection, and a word need to be fit into the proper part of speech to make sense in the sentence. 
    POS tagging is a very useful part of text preprocessing in NLP as we know that NLP is a task where we make a machine able to communicate with a human or with a different machine. So it becomes compulsory for a machine to understand the part of speech.
    Classifying words in their part of speech and providing their labels according to their part of speech is called part of speech tagging or POS tagging OR POST.
    
    \vspace{0.5cm}
    \subsection{POS tagging with Hidden Markov Model}
    
    Let's take an example to make it more clear how HMM helps in selecting an accurate POS tag for a sentence.
    Consider the sentence "The cat sat on the mat.", We want to determine the most likely POS for each word and the HMM model.
    
    Possible POS tags are: \hspace{1cm} $\bullet$ Noun (N) \hspace{1cm} $\bullet$ Verb (V) \hspace{1cm} $\bullet$ Determiner (D) \hspace{1cm} $\bullet$ Preposition (IN)
    
    HMM calculation: 
    
    $\bullet$ "The" is most likely a determiner (D)
    
    $\bullet$ Given that "The" is a D, "cat" is likely a N
    
    $\bullet$ Following a noun, "sits" is likely a verb
    
    $\bullet$ Similarly, "on" is a prespoition (IN) and "the" is again a determiner (D)
    
    $\bullet$ Finally, "mat" is most likely another noun (N)
    \vspace{1cm}

Example:

Let's illustrate a Hidden Markov Model (HMM) example for Part-of-Speech (POS) tagging, a common NLP task.  We'll keep it simple for demonstration.

\textbf{The Scenario:}

Imagine we have a simplified language with only two parts of speech: Noun (N) and Verb (V).  We observe sentences like "The cat sat" and want to tag each word with its correct POS.

\textbf{The HMM Components:}

States (Hidden): These are the POS tags we want to predict (N, V).
Observations (Visible): These are the words in the sentence ("The", "cat", "sat").
Transition Probabilities: The probability of moving from one state to another (e.g., the probability of a Noun following a Verb). We'll represent this as P(State_t | State_t-1).

Emission Probabilities: The probability of observing a particular word given a state (e.g., the probability of observing "cat" given the state Noun). We'll represent this as P(Observation_t | State_t).

Initial Probabilities: The probability of starting in a particular state (e.g., the probability of the first word being a Noun). We'll represent this as P(State_1).

\textbf{Example Probabilities (Simplified):}

Let's make up some probabilities for our example.  In a real-world scenario, these would be learned from a large corpus of tagged text.

Initial Probabilities:

P(Noun) = 0.6 (We assume nouns are more likely to start a sentence)

P(Verb) = 0.4

Transition Probabilities:

P(Noun | Noun) = 0.4 (A noun is sometimes followed by another noun, like in "red car")

P(Verb | Noun) = 0.6 (A noun is often followed by a verb)

P(Noun | Verb) = 0.3 (A verb is less likely to be followed by a noun)

P(Verb | Verb) = 0.7 (Verbs can be followed by other verbs, e.g., "run fast")

Emission Probabilities:

P("The" | Noun) = 0.1 (The word "The" can sometimes be a noun, although unlikely)

P("The" | Verb) = 0.0 (The word "The" is almost never a verb)

P("cat" | Noun) = 0.8 (The word "cat" is very likely a noun)

P("cat" | Verb) = 0.05 (The word "cat" could be a verb in some rare context)

P("sat" | Noun) = 0.05 (The word "sat" can sometimes function as a noun)

P("sat" | Verb) = 0.9 (The word "sat" is very likely a verb)

\textbf{Tagging the Sentence "The cat sat":}

Now, how do we use this to tag the sentence?  The Viterbi algorithm is commonly used for this.  It finds the most likely sequence of hidden states (POS tags) given the observed words.

Here's a simplified breakdown of how Viterbi would approach it (without the full dynamic programming table):

Start:  Calculate the probability of each possible first tag:

P(Noun | "The") = P("The" | Noun) * P(Noun) = 0.1 * 0.6 = 0.06
P(Verb | "The") = P("The" | Verb) * P(Verb) = 0.0 * 0.4 = 0.0
Noun is more likely for the first word.

Second Word ("cat"):  For each possible tag of "cat", consider the most likely previous tag:

For Noun: P(Noun | "cat") = P("cat" | Noun) * max(P(Noun | "The"), P(Verb | "The")) * P(Noun | Noun or Verb).
For Verb: P(Verb | "cat") = P("cat" | Verb) * max(P(Noun | "The"), P(Verb | "The")) * P(Verb | Noun or Verb).
We'd calculate these and see which tag for "cat" is more probable.

Third Word ("sat"): We repeat the process, considering the most likely tag for "cat" when calculating the probabilities for "sat" being a Noun or a Verb.

Backtracking:  After calculating probabilities for the last word, we backtrack through the highest probability paths to find the most likely sequence of tags.

\textbf{Expected Output:}

The Viterbi algorithm (or similar methods) would likely output:

"The" - Noun
"cat" - Noun
"sat" - Verb

Key Improvements over Simpler Methods:

HMMs handle ambiguity much better than simple lookup tables.  For example, "run" can be a noun or a verb.  The HMM considers the context (previous words and their tags) to determine the most likely POS.
    \subsection{Maximum Entropy in NLP}
    
    Maximum Entropy is a powerful statistical method frequently employed in Natural Language Processing for tasks like text classification, part-of-speech
    tagging, and named entity recognition. It's based the principle of maximizing uncertainty or entropy, subject to constraints derived from observed data.
	\subsection{Backpropagation in Neural Network}
	
	\vspace{0.5cm}
	
	Backpropagation (Backward Propagation of Errors) is a method used to train artificial neural networks. Its goal is
	
	 to reduce the difference between the model's predicted output and the actual output by adjusting the weights in
	 
	  the network.
	
	\vspace{0.5cm}
	
	\textbf{What is Backpropagation?}
	
	\vspace{0.5cm}
	
	Backpropagation is a powerful algorithm in deep learning, primarily used to train artificial neural networks. It 
	
	works iteratively, minimizing the cost function by adjusting weights and biases.
	
	\vspace{0.2cm}
	
	In each epoch, the model adapts these parameters, reducing loss by following the error gradient. Backpropagation
	
	 often utilizes optimization algorithms like gradient descent or stochastic gradient descent 
	
	\vspace{0.5cm}
	
	\subsection{Working of Backpropagation Algorithm}
	
	\vspace{0.5cm}
	
	The Backpropagation algorithm involves two main steps: the \textbf{Forward Pass} and the \textbf{Backward Pass}
	
	\vspace{0.3cm}
	
	\subsection{How does the Forward Pass Work?}
	
	\vspace{0.3cm}
	In the forward pass, the input data is fed into the input layer. These inputs, combined with their respective 
	
	weights, are passed to hidden layers.
	
	For example, in a network with two hidden layers (h1 and h2 as shown in next page), the output from h1 serves as
	
	 the input to h2. Before applying an activation function, a bias is added to the weighted inputs.
	
	Each hidden layer applies an activation function like ReLU (Rectified Linear Unit), which returns the input if it’s
	
	 positive and zero otherwise. This adds non-linearity, allowing the model to learn complex relationships in the data.
	 
	  Finally, the outputs from the last hidden layer are passed to the output layer, where an activation function,
	  
	   such as softmax, converts the weighted outputs into probabilities for classification.
	   
    \vspace{0.5cm}
    
    \subsection{How does the Backward Pass Work?}
    
    \vspace{0.3cm}
    
    In the backward pass, the error (the difference between the predicted and actual output) is propagated back through the network to adjust the weights and biases. One common method for error calculation is the Mean Squared Error (MSE), given by:
    \begin{equation}
    MSE = (Predicted \hspace{0.1cm} Output - Actual \hspace{0.1cm} Output)^2
\end{equation}
    
    Once the error is calculated, the network adjusts weights using gradients, which are computed with the chain rule. These gradients indicate how much each weight and bias should be adjusted to minimize the error in the next iteration. The backward pass continues layer by layer, ensuring that the network learns and improves its performance. The activation function, through its derivative, plays a crucial role in computing these gradients during backpropagation.
\section{Formal Grammars in Natural Language Processing: A Foundation for Understanding Language Structure}
	Formal grammars are a cornerstone of Natural Language Processing (NLP), providing a precise and mathematical way to describe the structure of language.  They offer a framework for understanding how words combine to form phrases, clauses, and sentences, enabling computers to parse and interpret human language.  While natural language is often messy and defies strict rules, formal grammars provide valuable abstractions that capture key syntactic patterns and serve as the basis for many NLP applications.   
	
	\subsection{The Basics of Formal Grammars:}
	
	A formal grammar, in its most basic form, consists of:
	
	A set of terminals: These are the actual words or symbols that make up the language (e.g., "cat," "the," "sat," "on," "mat").   
	A set of non-terminals: These are symbols that represent grammatical categories or syntactic constituents (e.g., "Noun," "Verb," "Adjective," "Noun Phrase," "Verb Phrase," "Sentence").   
	A set of production rules: These rules define how non-terminals can be rewritten as combinations of terminals and/or other non-terminals. They express the grammatical relationships within the language. A common notation for production rules is A → B, where A is a non-terminal and B is a sequence of terminals and/or non-terminals.   
	A start symbol: This is a special non-terminal that represents the top-level structure of the language, typically a "Sentence" (S).   
	A grammar generates a language by starting with the start symbol and repeatedly applying the production rules until only terminals remain.  The set of all strings of terminals that can be derived in this way constitutes the language defined by the grammar.   
	
	\textbf{Example:}
	
	Consider a simple grammar for a subset of English:
	
	Terminals: {the, cat, sat, on, mat}
	
	Non-terminals: {S, NP, VP, N, V, P}
	
	Production rules:
	
	S → NP VP
	
	NP → Det N
	
	NP → N
	
	VP → V NP
	
	VP → V P NP
	
	Det → the
	
	N → cat
	
	N → mat
	
	V → sat
	
	P → on
	
	Start symbol: S
	
	Using this grammar, we can derive the sentence "The cat sat on the mat" as follows:
	
	S → NP VP
	
	→ Det N VP
	
	→ the N VP
	
	→ the cat VP
	
	→ the cat V P NP
	
	→ the cat sat P NP
	
	→ the cat sat on NP
	
	→ the cat sat on Det N
	
	→ the cat sat on the N
	
	→ the cat sat on the mat   
	
	\subsection{Types of Formal Grammars:}
	
	Formal grammars are often classified according to the Chomsky hierarchy, which defines a hierarchy of grammar types based on the complexity of their production rules:   
	
	Regular Grammars (Type 3): These are the simplest type of grammar, where production rules are of the form A → aB or A → a, where A and B are non-terminals and 'a' is a terminal. Regular grammars can be recognized by finite automata. They are often used for tasks like morphological analysis and tokenization.   
	Context-Free Grammars (Type 2): In CFGs, production rules are of the form A → α, where A is a non-terminal and α is a string of terminals and/or non-terminals. The left-hand side of the rule consists of a single non-terminal, and the right-hand side can be any combination of terminals and non-terminals. CFGs are powerful enough to capture many of the syntactic structures of natural language and are widely used in parsing. They can be recognized by pushdown automata.   
	Context-Sensitive Grammars (Type 1): These grammars allow for more complex rules, including rules of the form αAβ → αγβ, where A is a non-terminal, α and β are strings of terminals and/or non-terminals (possibly empty), and γ is a non-empty string of terminals and/or non-terminals. The context in which A appears (α and β) can influence how it is rewritten. Context-sensitive grammars are more powerful than CFGs but are less commonly used in NLP due to their increased complexity.
	Recursively Enumerable Grammars (Type 0): These are the most general type of grammar, with no restrictions on the form of production rules. They can generate any language that can be recognized by a Turing machine. While theoretically powerful, they are rarely used in practice due to their extreme generality and the difficulty of parsing them.   
	In NLP, Context-Free Grammars are the most widely used due to their balance between expressive power and computational tractability.
	
	\subsection{Formal Grammars in NLP Applications:}
	
	Formal grammars play a crucial role in various NLP tasks:
	
	Parsing: Parsing is the process of analyzing a sentence according to a grammar to determine its syntactic structure. Parsers use formal grammars to build parse trees, which represent the hierarchical organization of the sentence. This information is essential for understanding the meaning of the sentence. Several parsing algorithms exist, including top-down parsing, bottom-up parsing, and chart parsing.   
	Machine Translation: Formal grammars can be used to analyze the syntactic structure of the source language and generate the corresponding structure in the target language. This helps to ensure that the translated sentence is grammatically correct and preserves the meaning of the original sentence.   
	Grammar Checking: Grammar checkers use formal grammars to identify grammatical errors in text. By comparing the structure of the input text to the rules of the grammar, they can detect violations of grammatical rules.   
	Dialogue Systems: Formal grammars can be used to define the possible inputs that a dialogue system can understand. This allows the system to parse user input and respond appropriately.   
	Information Extraction: Formal grammars can be used to identify specific entities and relationships in text. For example, they can be used to extract person names, locations, and dates from news articles.   
	
	\subsection{Limitations and Extensions:}
	
	While formal grammars provide a valuable framework for understanding language structure, they also have limitations:
	
	Ambiguity: Natural language is often ambiguous, meaning that a single sentence can have multiple possible interpretations. Formal grammars need to be able to handle ambiguity and provide all possible parses.   
	Coverage: Creating a grammar that covers all the complexities and irregularities of natural language is a difficult task. Grammars often need to be extended and refined to handle new linguistic phenomena.
	Statistical Methods: Purely grammar-based approaches often struggle with the inherent variability and noise in natural language. Statistical methods, such as probabilistic context-free grammars (PCFGs), have been developed to address this issue by assigning probabilities to different parse trees. These probabilities can be learned from training data and used to select the most likely parse.   
	
	

